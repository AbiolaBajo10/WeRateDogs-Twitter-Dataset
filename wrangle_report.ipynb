{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA WRANGLING REPORT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROJECT TITLE:  Wrangle and Analyze WeRateDogs Tweet Archive Data\n",
    "\n",
    "### PROJECT AIM: \n",
    "\n",
    "The aim of this project is to wrangle (gather, assess, clean) the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. After wrangling, analysis and visualization is to be done on the dataset.\n",
    "\n",
    "### INTRODUCTION \n",
    "\n",
    "The dataset for this project is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. To perform an analysis, the following datasets was needed:\n",
    "1. The WeRateDogs Twitter archive\n",
    "2. The tweet image predictions\n",
    "3. Additional data from the Twitter API(retweet_count, favorite_count)\n",
    "\n",
    "### GATHERING\n",
    "\n",
    "#### The WeRateDogs Twitter archive\n",
    "\n",
    "The WeRateDogs Twitter archive data was provided by Udacity as 'twitter_archive_enhanced.csv' file. I downloaded the data manually, uploaded it locally on the jupyter notebook and read it into a pandas DataFrame (twitter_df).\n",
    "\n",
    "#### The tweet image prediction\n",
    "\n",
    "This file (image_predictions.tsv) is present in each tweet according to a neural network. It is hosted on Udacity's servers and was downloaded programmatically using the Requests library and a URL provided by Udacity. This dataset was read into a pandas dataframe (image_df).\n",
    "\n",
    "#### Additional data from the Twitter API(retweet_count, favorite_count)\n",
    "\n",
    "Each tweet's retweet count and favorite (\"like\") count was gathered using the tweet IDs in the WeRateDogs Twitter archive. I queried twitter API for each tweet's JSON data using Python's Tweepy library and each tweet's entire set of JSON data  was stored in a file called tweet_json.txt file.\n",
    "\n",
    "Each tweet's JSON data was written to its own line and read into a .txt file line by line. Finally, I read this .txt file into a pandas DataFrame (tweet_count) with (at minimum) tweet ID, retweet count, and favorite count.\n",
    "\n",
    "### ASSESSING\n",
    "\n",
    "The three datasets were assessed in two ways which are the visual assesssment and programmatic assessment.\n",
    "#### 1. Visual Assessment -\n",
    "\n",
    "I opened the twitter_df (twitter_archive_enhanced dataframe) in a spreadsheet, printed the image_df and tweet_count in the jupyter notebook. I scrolled through the columns and rows of these dataframes to find issues present.\n",
    "\n",
    "#### 2. Programmatic Assessment - \n",
    "\n",
    "The three dataframes were assessed programmatically using some pandas method such as .head(), .tail(), .info(), . describe(), .sample(), .duplicated(), .isnull(), and attributes such as .shape. I identified quality issues and tidiness isues. The following were observed from the programmatic assessment on the three dataframes.\n",
    "\n",
    "#### Quality Issues\n",
    "\n",
    "twitter_df table\n",
    "- Retweets present in the dataset.\n",
    "- Missing values in some columns.\n",
    "- Erroneous datatype in the timestamp column.\n",
    "- Nulls represented as None in the name column.\n",
    "- Incorrect ratings in rows 313, 516 and 2335. \n",
    "- The name column consists of lowercase names.\n",
    "\n",
    "image_df table\n",
    "- Non-descriptive column names.\n",
    "- The columns: p1, p2 and p3 have lowercase variables and uppercase variables\n",
    "\n",
    "#### Tidiness Issues\n",
    "\n",
    "twitter_df\n",
    "- The columns doggo, floofer, pupper and puppo should be in a single column 'dog types'\n",
    "- Redundant columns: text column.\n",
    "\n",
    "twitter_df, image_df and tweet_count\n",
    "- The three dataframes should be in one dataframe.\n",
    "\n",
    "### CLEANING\n",
    "\n",
    "Before cleaning, I made copies of the three dataframes.I defined the issue, wrote the cleaning code and tested the cleaning code to know if it worked. \n",
    "The issues that were assessed in the second wrangling step were cleaned in the following ways:\n",
    "- I dropped the retweets in the first dataframe because we only need the original tweets\n",
    "- I dropped the columns with missing values.\n",
    "- The tidiness issues were cleaned next. \n",
    "- Followed by the quality issues which were properly cleaned.\n",
    "- Lastly, the general tidiness issue was cleaned by merging the three dataframes.\n",
    "\n",
    "The merged dataframe was stored in a csv file named \"twitter_archive_master.csv\".\n",
    "\n",
    "### CONCLUSION\n",
    "\n",
    "The three datasets was effectively wrangled and merged into a single dataframe for analysis and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
